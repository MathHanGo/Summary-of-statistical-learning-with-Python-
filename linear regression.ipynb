{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression summary\n",
    "\n",
    "Linear regression is a method to predict quantitative response on the basis of a single predictor or multiple predictors by holding the assumption of a linear relationship between response and predictors. For each predictor, we give a coefficient to describe its weight. \n",
    "\n",
    "If we only have one predictor ${\\bf x}$ and the response ${\\bf y}$. Then the linear relationship between them can be formed as ${\\bf y} := \\beta_0 + \\beta_1{\\bf x}$ ( We give a weight $\\beta_1$ for ${\\bf x}$ to describe how ${\\bf x}$ close to ${\\bf y}$. Obvioulsy, if ${\\bf x}=0$, then ${\\bf y} = \\beta_0$, ${\\bf x}$ have no relationship with ${\\bf y}$). When we have more than one predictor, ${\\bf y} := \\beta_0 + \\beta_1{\\bf x}_1 + \\beta_2{\\bf x}_2+\\cdots+\\beta_p{\\bf x}_p$. For generalization, we form linear regression model as $f({\\bf x}) = \\beta_0 + \\sum_{i=1}^{p}{\\bf x_i}\\beta_i$. \n",
    "\n",
    "Our purpose is to predict ${\\bf Y}$ if given ${\\bf X}=[{\\bf 1},{\\bf x_1}, {\\bf x_2}, \\cdots, {\\bf x_p}]$. Thus, the first task is to estimate unknown coefficients $\\beta$ ( $\\beta=[\\beta_0, \\beta_1, \\cdots, \\beta_p]^T$). We may estimate the coefficient by a set of training date $({\\bf x_1},{\\bf y_1}), \\cdots, ({\\bf x_p},{\\bf y_p})$, where each ${\\bf x_i} = (x_{i1}, x_{i2},\\cdots, x_{ip})$. Noticed that each observation pair is assumed as an independent random draws from their population. Once we got the estimates $\\hat{\\beta}$, the prediction of ${\\bf Y}$ can be computed by $\\hat{\\bf Y} = {\\bf X}\\hat\\beta$.\n",
    "\n",
    "The most common method of coefficient estimation is __least squares__, which is to obtain $\\beta$ by minimizing the __residual sum of squares__. So what is the residual sum of square (RSS)?\n",
    "\n",
    "We define RSS as $RSS = \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}x_j\\beta_j)^2$ or a quadratic function $RSS(\\beta)=({\\bf y}-{\\bf X}\\beta)^T({\\bf y}-{\\bf X}\\beta)$, where ${\\bf X}$ is a $n \\times (p+1)$ matrix, $n$ refers to the number of observations and $p+1$ refers to the number of parameters. Differentiating the quadratic function with respect to $\\beta$ and let it euqal to $0$, we got:\n",
    "\n",
    "${\\bf X}^T({\\bf y} - {\\bf X}\\beta) = 0$. \n",
    "\n",
    "Thus,\n",
    "\n",
    "$\\hat{\\beta} = ({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf y}$. Noticed that the columns of ${\\bf X}$ is linearly independent so that it is a full-rank matrix. In this case, ${\\bf X}^T{\\bf X}$ is positive definite and thus we can calculate $\\hat{\\beta}$ directly. ($\\hat{\\beta}$ should be under the same distribution as ${\\bf y}$)\n",
    "\n",
    "We assume that the true relationship between predictors and response takes the from ${\\bf y} = f({\\bf x}) + \\epsilon$, where $\\epsilon \\backsim N(0,\\sigma^2)$. Therefore, ${\\bf y}$ is also random varibale of normal distribution and $Var({\\bf y}) = Var(\\epsilon) = \\sigma^2$.\n",
    "\n",
    "Then, we know that $Var(\\hat{\\beta}) = Var(({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf y}) = ({\\bf X}^T{\\bf X})^{-1}{\\bf X}^TVar({\\bf y}){\\bf X}({\\bf X}^T{\\bf X})^{-1}=({\\bf X}^T{\\bf X})^{-1}\\sigma^2$. \n",
    "\n",
    "$E(\\hat{\\beta})= E(({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf y})=E(({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf X}{\\beta})=\\beta$\n",
    "\n",
    "Thus, ${\\hat\\beta} \\backsim N(\\beta,({\\bf X}^T{\\bf X})^{-1}\\sigma^2)$ and this is a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example in the book ___Machine Learning in Action___. We first take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067732</td>\n",
       "      <td>3.176513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.427810</td>\n",
       "      <td>3.816464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995731</td>\n",
       "      <td>4.550095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738336</td>\n",
       "      <td>4.256571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.981083</td>\n",
       "      <td>4.560815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2\n",
       "0  1.0  0.067732  3.176513\n",
       "1  1.0  0.427810  3.816464\n",
       "2  1.0  0.995731  4.550095\n",
       "3  1.0  0.738336  4.256571\n",
       "4  1.0  0.981083  4.560815"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('D:/math/book/data mining/ML/machinelearninginaction/Ch08/ex0.txt', sep='\\t',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200L, 2L)\n"
     ]
    }
   ],
   "source": [
    "dataMat=df.loc[:,0:1].as_matrix()\n",
    "print(dataMat.shape)\n",
    "label=df[2].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standReg(x,y):\n",
    "    xTx=np.dot(x.T,x)\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print('The inverse matrix does not exist')\n",
    "        return\n",
    "    return np.dot(np.linalg.inv(xTx), np.dot(x.T,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got the estimates of beta:\n",
      "[ 3.00774324  1.69532264]\n"
     ]
    }
   ],
   "source": [
    "beta_=standReg(dataMat,label)\n",
    "print('We got the estimates of beta:')\n",
    "print(beta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xad15828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(dataMat[:,1],label)\n",
    "plt.plot(dataMat[:,1],np.dot(dataMat, beta_), color='r')\n",
    "fig=plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have only one predictors. It can be see all the values of the first column of _dataMat_ are 1. Because ${\\bf X}\\beta=[1 \\ {\\bf x_1}][\\beta_0 \\ \\beta_1]^T = \\beta_0 + \\beta_1{\\bf x_1}$. Thus we got $\\beta_0 = 3.00774324$ and $\\beta_1=1.69532264$. It can be seen that we make use of all observations to train the coefficients. But in general we would not do this.\n",
    "\n",
    "In general, we divide the data into two set, traning data and test data. The following example uses __sklearn__ library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV, Ridge, LassoCV, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('D:/math/book/data mining/ML/dataset/CCPP/Folds5x2_pp.csv')\n",
    "x = df[['AT', 'V', 'AP', 'RH']]\n",
    "y = df[['PE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(x,y):\n",
    "    dataMat_train, dataMat_test, label_train, label_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    linreg = linear_model.LinearRegression(fit_intercept =True) #We do not fit intercept becasue the first column of dataMat is 1\n",
    "    linreg.fit(dataMat_train,label_train)\n",
    "    print(linreg.coef_)\n",
    "    \n",
    "    label_pred = linreg.predict(dataMat_test)\n",
    "    mse = mean_squared_error(label_test, label_pred)\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.97852801 -0.23403882  0.05833255 -0.15906102]]\n",
      "20.0714469653\n"
     ]
    }
   ],
   "source": [
    "linear_regression(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More about linear regression\n",
    "\n",
    "The __least squares__ produces the estimates of $\\beta$ for getting future respones with ${\\bf x}$. This method gives us estimates often have low bias but high variance. It is not what we expect. We expect an estimate with low bias and low variace. By doing so we sacrifice a liitle bit of bias to reduce the variance of the preicted value. On the other hand, least squares make use all of the predictors. But when the number of predictors is large, we prefer a smaller subset predictors which exhibit the strongest effects.\n",
    "\n",
    "__First__, we see how close $\\hat{\\bf Y}$ to ${\\bf Y}$.\n",
    "\n",
    "If $\\mu$ is the mean of ${\\bf Y}$, we use sample mean $\\hat\\mu$ to estimate the $\\mu$. We always hope that the estimate is unbias such that $E(\\hat\\mu) = \\mu$. Then how far off will that single estimate of $\\hat\\mu$ be? We could answer this question by computing the variance of $\\hat\\mu$.\n",
    "\n",
    "Since $Var(\\hat\\mu)=Var(\\frac{1}{n}\\sum_{i=1}^{n}y_i)=\\frac{1}{n^2}Var(\\sum_{i=1}^{n}y_i)=\\frac{1}{n^2}\\ n\\sigma^2=\\frac{\\sigma^2}{n}$. It tells us, the large the number of observations $n$, the less the value of variance of $\\hat\\mu$. Thus, we conclude that if we want to reduce the variance, one of the method is to increase the number of observations.\n",
    "\n",
    "__Sechond__, we give two shinkage methods which could reduce the variance, __Rigdge Regression__ and __The Lasso__\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Ridge regression estimate the coefficient by minimizing a penalized RSS:\n",
    "\n",
    "$\\begin{equation}\n",
    "    RSS(\\beta_{rigde})=({\\bf y}-{\\bf X}\\beta)^T({\\bf y}-{\\bf X}\\beta) + \\lambda\\beta^T\\beta\n",
    "\\end{equation}$\n",
    "\n",
    "where $\\beta^T\\beta$ is the a norm-2 regularization. $\\lambda$ is the regularization parameter to balance two terms. Differentiating the $RSS(\\beta)$ with respect to $\\beta$ and let it euqal to $0$, we got $\\hat\\beta_{rigde}=({\\bf X}^T{\\bf X} + \\lambda{\\bf I})^{-1}{\\bf X}^T{\\bf y}$. Then $Var(\\hat{\\bf y}_{ridge})=Var({\\bf X}\\hat{\\beta}_{ridge}) = {\\bf X}({\\bf X}^T{\\bf X} + \\lambda{\\bf I})^{-1}{\\bf X}^T\\sigma^2$.\n",
    "\n",
    "Comparing with the variance of $Var(\\hat{\\bf y}) = {\\bf X}({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T\\sigma^2$, we see that the variance of estimate is reduced.\n",
    "\n",
    "Notied that different $\\lambda$ produce a different set of $\\beta$. Thus, a good value for $\\lambda$ will be selected by cross-validation.\n",
    "\n",
    "Another thing in ridge regression. Different scaling of predictor will not effect ${\\bf X}_j\\hat\\beta_j$ in least squares. But it has impace in ridge regression because the $L_2$ penalty. Thus, it is best to apply ridge regression after standardizing the predictors.\n",
    "\n",
    "### The Lasso\n",
    "\n",
    "Instead of $L_2$ penlty, the lasso uses $L_1$ penlty. Its estimate is defined by $\\hat\\beta_lasso = argmin_{\\beta}\\{\\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\beta_0 -\\sum_{j=1}^{p}x_ij\\beta_j)^2 + \\lambda\\sum_{j=1}^{p}|\\beta_j|\\}$. This latter constraint is non-convex, and there is no closed form expression. Although the lasso shrinks the coefficient estimates towards zero. But the $L_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\\lambda$ is sufficiently large. Thus, the lasso performs variable selection.\n",
    "\n",
    "There are two algorithm to solve the lasso, coordinate descent and least angle regression.(This part will be added later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(x,y):\n",
    "    dataMat_train, dataMat_test, label_train, label_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    #select the value of parameter by cross validation.\n",
    "    alpha_Mat= np.logspace(-10,10,10)\n",
    "    ridgecv = RidgeCV(alphas=alpha_Mat, fit_intercept=True,cv=10)\n",
    "    ridgecv.fit(dataMat_train, label_train)\n",
    "    \n",
    "    ridge = Ridge(alpha=ridgecv.alpha_,fit_intercept=True)\n",
    "    ridge.fit(dataMat_train,label_train)\n",
    "    print(ridgecv.alpha_)\n",
    "    \n",
    "    return ridge.coef_ , mean_squared_error(label_test, ridge.predict(dataMat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9154966501\n",
      "[[-1.9781638  -0.23417727  0.05842311 -0.15899963]]\n",
      "20.0714198433\n"
     ]
    }
   ],
   "source": [
    "a,b=ridge_regression(x,y)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lasso_regression(x,y):\n",
    "    dataMat_train, dataMat_test, label_train, label_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    alpha_Mat= np.logspace(-10,10,10)\n",
    "    lassocv=LassoCV(alphas=alpha_Mat,fit_intercept=True,cv=10)\n",
    "    lassocv.fit(dataMat_train, label_train)\n",
    "    \n",
    "    lasso=Lasso(alpha=lassocv.alpha_,fit_intercept=True)\n",
    "    lasso.fit(dataMat_train,label_train)\n",
    "    print(lassocv.alpha_)\n",
    "    return lasso.coef_, mean_squared_error(label_test, lasso.predict(dataMat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000464158883361\n",
      "[-1.97858225 -0.23401902  0.05830488 -0.15906742]\n",
      "20.07145931\n"
     ]
    }
   ],
   "source": [
    "a,b=lasso_regression(x,y)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
