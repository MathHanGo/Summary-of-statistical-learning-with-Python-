{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the binary responses, we have two classes, say 0 and 1. We want to estimate the conditional probability of $Pr(1| X)$. Given $X$, if $Pr(1|X) > 0.5$, then we can say they are falling into the class 1. Otherwise, they may belong to the class 0. How to form the relationship between $X$ and $Pr(1| X)$?\n",
    "\n",
    "### Logistic regression\n",
    "If we start from linear regression, we can still predict that $Pr(1| X) = {\\bf X}\\beta$. However, the value of $Pr(1| X)$ may be larger than $1$ or negative. In order to aviod this problem, we have to model $Pr(1| X)$ by a function that gives outputs in the interval of $[0,1]$. If we use the _logistic function_, then we got logistic regression.\n",
    "\n",
    "Logistic regression is formed as $p(x)=Pr(1| X)=\\frac{e^{\\beta_0+\\beta_1x_1+\\cdots+\\beta_nx_n}}{1+e^{\\beta_0+\\beta_1x_1+\\cdots+\\beta_nx_n}}$.\n",
    "\n",
    "Here, I perfer to understand the logistic regression by the decision boundary.\n",
    "\n",
    "For the binary respones, the decision boundary is a set of points for $Pr(1|X)=Pr(0|X)$. Or setting the equation like this, $log(\\frac{Pr(1|X)}{Pr(0|X)})=0$ (__log-odds__). From the logistic regression expressed above, we could form a liner log-odds:\n",
    "\n",
    "$log odds = log(\\frac{Pr(1|X)}{Pr(0|X)}) = \\beta_0+\\beta_1x_1+\\cdots+\\beta_nx_n $\n",
    "\n",
    "Thus the decision boundary is the set of points for which the log-odds are zero, or hyperplane defined by $\\{x|\\beta_0+\\beta_1x_1+\\cdots+\\beta_nx_n\\}$.\n",
    "\n",
    "##### Fitting logistic regression models\n",
    "We still need to estimate the coefficient. In general, this model is fitted by _maximum likelihood_.\n",
    "\n",
    "For the two-class case, $Pr(1|X)=1-Pr(0|X)$. The log-likelihood function can be written as $l(\\beta)=\\sum_{i=1}^{n}\\{y_ilog(p(x_i))+(1-y_i)log(1-p(x_i)\\}$. To estimate a set of $\\beta$, we have to maximum the log-likelihood function.\n",
    "\n",
    "Setting the dervative respectives to $\\beta$ of the log-likehood function to zero, we got $\\sum_{i=1}^{n}x_i(y_i-p(x))=0$. To address this problem, we use the _Newton-Raphson_ algorithm and thus we have to know the second-derivative which is $-\\sum_{i=1}^{n}x_ix_i^{T}p(x_i)(1-p(x_i))$. For the convenece, we re-write the first-derivative and the second-deriviative in matrix notation.\n",
    "\n",
    "The first dervative: ${\\bf X}^T({\\bf y}-{\\bf p})$\n",
    "\n",
    "The second dervative: $-{\\bf X}^T{\\bf W}{\\bf X}$ where ${\\bf W}$ is a $n\\times n$ diagonal matrix of weights with $i$th diagonal element $p(x_i)(1-p(x_i))$.\n",
    "\n",
    "The Newton step at $j$th iterative is:\n",
    "\n",
    "$\\beta^{(j+1)}=\\beta^{(j)}+({\\bf X}^T{\\bf W}{\\bf X})^{-1}{\\bf X}^T({\\bf y} - {\\bf p})$\n",
    "\n",
    "_(stop here, may add more details about newton methods later)_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(x,y):\n",
    "        dataMat_train, dataMat_test, label_train, label_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "        \n",
    "        logsiticreg=LogisticRegression()\n",
    "        logsiticreg.fit(dataMat_train,label_train)\n",
    "        \n",
    "        predictions=logsiticreg.predict(dataMat_test)\n",
    "        \n",
    "        score=logsiticreg.score(dataMat_test,label_test)\n",
    "        cm=metrics.confusion_matrix(label_test, predictions)\n",
    "        return score,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n",
      "[[8 4]\n",
      " [4 9]]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('../testSetRBF2.txt',sep='\\t',header=None)\n",
    "x=df.iloc[:,0:2]\n",
    "y=df.iloc[:,2]\n",
    "score1,cm1 = logistic_regression(x,y)\n",
    "print(score1)\n",
    "print(cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n",
      "[[42  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  1  0]\n",
      " [ 0  0 38  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 44  0  1  0  0  1  0]\n",
      " [ 0  1  0  0 54  0  0  0  0  0]\n",
      " [ 0  0  1  0  0 56  0  0  1  1]\n",
      " [ 0  0  0  0  0  1 44  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 40  0  1]\n",
      " [ 0  1  0  0  0  1  0  0 36  0]\n",
      " [ 0  0  0  0  0  0  0  0  3 45]]\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "score2,cm2=logistic_regression(digits.data,digits.target)\n",
    "print(score2)\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
